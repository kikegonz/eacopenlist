# -*- coding: utf-8 -*-

import csv
import subprocess
import nltk
from nltk.corpus.reader import ConllChunkCorpusReader
#log management
import sys
import time
import datetime

#########################################################################
# Based on:                                                             #
# http://www.nltk.org/book/ch07.html Chunker & Relation Extractions     #
# The inputs are the CSV generated by the scrapy spiders                #
# The output will be a CSV file with the fields to introduce in DB      #
#########################################################################


#########################################################################
# Chunker trainer definition
# Importing and initializing the Corpus. Each of the files should begin by "trainer_"
conllreader = ConllChunkCorpusReader('conlleac/', 'trainer_.*', ('VND', 'PDT', 'VSN'))


# Trainer function
class TrigramChunker(nltk.ChunkParserI):
    def __init__(self, train_sents):
        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]
        self.tagger = nltk.TrigramTagger(train_data)

    def parse(self, sentence):
        pos_tags = [pos for (word, pos) in sentence]
        tagged_pos_tags = self.tagger.tag(pos_tags)
        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]
        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]
        return nltk.chunk.conlltags2tree(conlltags)
###                                                                   ###
#########################################################################


def chunker(text, chunker_trainer):
    #Input arguments: The text to analyze and the information if we are chunking the web page title or body (raw text).
    #The Output is the list of trees with chunks labeled according the corpus defined in conllreader
    text = text.decode('utf8')  # to avoid undesired characters
    sentences = nltk.sent_tokenize(text)
    sentences = [nltk.word_tokenize(sent) for sent in sentences]
    if chunker_trainer == "product":
        #trainer_product trains with the goal of recognize NE inside visited pages titles
        train_sents_product = conllreader.chunked_sents('trainer_product', chunk_types=['VND', 'PDT', 'VSN'])
        chunkerTrigram = TrigramChunker(train_sents_product)
        #We use the same ConllChunkCorpusReader corpus to train a tagger
        train_tagger_product = conllreader.tagged_sents('trainer_product')
        tagger_t0 = nltk.DefaultTagger('NNP')
        tagger_t1 = nltk.UnigramTagger(train_tagger_product, backoff=tagger_t0)
    #Lets tag the text sentences
    sentences = [tagger_t1.tag(sent) for sent in sentences]
    #With the sentences tagged lets take the NE
    output = []
    for sent in sentences:
        output.append(chunkerTrigram.parse(sent))
    return output

#####################################################################################################
#MAIN FUNCTION
#####################################################################################################
#Opening log file and routing the stdout and stderr
saveout = sys.stdout
fsock = open('../eacopenlistbot/log/chunker_rel_extractor.log', 'a')
sys.stdout = fsock
sys.stderr = fsock

#Opening all the CSV files at the work directory
csv_list = subprocess.check_output('ls ../eacopenlistbot/*.csv', shell=True, )
csv_list = csv_list.split('\n')

#Getting ready the output file
csv_output = csv.writer(open('../eacopenlistbot/info_output.csv', 'w+'))
for csv_file in csv_list:
    csv_input = csv.reader(file(csv_file))
    #We save the source in order to prioritize results according the reliability of the web source information
    out_source = csv_file
    start_row = 1
    cur_row = 0
    for row in csv_input:
        #Intializing the lists of items to save from the crawled data.
        product = []
        vendor = []
        version = []
        if cur_row >= start_row:
            out_vendor = ""
            out_product = ""
            out_ftr = []
            if row[0]:
                #We'll trust on market website to identify the vendor'
                out_vendor = row[0]
            if row[1]:
                chunker_trainer = "product"
                #Please chunk the text in row[1] that is the product column of the csv (web page title)
                pre_product = chunker(row[1], chunker_trainer)
                #we save the items chunked in its respective lists
                for tree in pre_product:
                    for subtree in tree:
                        #Not every subtree has label atribute. We have to manage that exception
                        try:
                            if subtree.label() == "VND":
                                for leave in subtree.leaves():
                                    if len(vendor) == 0:
                                        vendor.append(leave[0])
                            if subtree.label() == "PDT":
                                for leave in subtree.leaves():
                                    product.append(leave[0])
                            if subtree.label() == "VSN":
                                for leave in subtree.leaves():
                                    version.append(leave[0])
                        except:
                            pass
                    #Lets Join the list to conform the results
                    if not out_vendor:
                        out_vendor = " ".join(vendor)
                    out_product = " ".join(product)
                    out_version = " ".join(version)
            '''
            if row[2]:
                #Pending of trainer corpus to identify features at the web bodies (row[2])
            '''
        cur_row += 1
