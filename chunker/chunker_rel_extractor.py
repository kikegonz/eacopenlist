# -*- coding: utf-8 -*-
import csv
import subprocess
import nltk
#log management
import sys
import time
import datetime

#########################################################################
# http://www.nltk.org/book/ch07.html Chunker & Relation Extractions     #
# The inputs are the CSV generated by the scrapy spiders                #
# The output will be a CSV file with the fields to introduce in DB      #
#########################################################################


def chunker(text):
    #http://www.nltk.org/book/ch07.html   1.1 Information extraction Architecture
    text = text.lower()  # to avoid incorrect NNP tags
    text = text.decode('utf8')  # to avoid undesired characters
    sentences = nltk.sent_tokenize(text)
    sentences = [nltk.word_tokenize(sent) for sent in sentences]
    #We choose the NLTK's recommended tagger since any other tagger doesn't avoid
    #the RegexpParse grammar options in spite of their better accuracy
    sentences = [nltk.pos_tag(sent) for sent in sentences]

    #In order to control the tags assigned by pos_tag and not to complicate the RegexpParse grammar options
    #We limit the pos tags options to a several included at the 'allowed' list
    #If we find a not 'allowed' tag we change it by NNP or we avoid the whole tuple
    #Since pos_tag ouput is a list of tuples (Return type: list(tuple(str, str))) we have to
    #create a new list of tuples
    output = []
    sentences2 = []
    allowed = ['NN', 'NNP', 'NNS', 'VBD', 'VBN', 'CD', 'JJ']
    notallowed = ['$', '"', '(', ')', ',', '--', '.', ':', "''"]
    for sent in sentences:
        sent2 = []
        #Since we use tuple() function we can use the word tuple as variable name
        for tupl in sent:
            tupl2 = []
            if tupl[1] in notallowed:
                pass  # do nothing. In order not to add punctuation marks
            elif tupl[1] not in allowed:
                tupl2.append(tupl[0])
                tupl2.append('NNP')
            else:
                tupl2.append(tupl[0])
                tupl2.append(tupl[1])
            if len(tupl2) > 0:
                tupl2 = tuple(tupl2)
                sent2.append(tupl2)
        sentences2.append(sent2)

    #Chunk Function Options
    #<> delimit an element
    #∗  zero or more occurrences, e.g., <JJ>∗
    #+  one or more occurrences, e.g., <NN>+
    #|  alternatives, e.g., IN|TO
    #?  the previous character is optional, e.g., NNS?
    grammar = r"""
        NP: {<NN>?<VBD|VBN>+} # ie. "factory/NN unlocked/VBD"
            {<JJ>+<NN|NNP|NNS>?} # ie. "dual/JJ sim/NN" or "black/JJ"
            {<NN|NNP|NNS>+<CD>+<NN|NNP|NNS>?} # ie. "lg/NP nexus/NN 5/CD D820/NNP"
            {<NN|NNP|NNS>+} # ie. "samsung/NNP galaxy/NN s4/NNP i9500/NNP"
            {<CD>+} # ie. "16gb/CD"
    """
    cp = nltk.RegexpParser(grammar)
    for sent2 in sentences2:
        output.append(cp.parse(sent2))
    return output


#####################################################################################################
#MAIN FUNCTION
#####################################################################################################

#Opening log file and routing the stdout and stderr
saveout = sys.stdout
fsock = open('/home/quique/labo/eacopenlist/eacopenlistbot/log/chunker_rel_extractor.log', 'a')
sys.stdout = fsock
sys.stderr = fsock

#Opening all the CSV files at the work directory
csv_list = subprocess.check_output('ls /home/quique/labo/eacopenlist/eacopenlistbot/*.csv', shell=True, )
csv_list = csv_list.split('\n')

csv_output = csv.writer(open('/home/quique/labo/eacopenlist/eacopenlistbot/info_output.csv', 'w+'))
for csv_file in csv_list:
    csv_input = csv.reader(file(csv_file))
    source = csv_file
    start_row = 1
    cur_row = 0
    for row in csv_input:
        if cur_row >= start_row:
            if row[0]:
                vendor = row[0]
            if row[1]:
                pre_product = chunker(row[1])
            #csv_output.writerow(source, vendor)
        cur_row += 1